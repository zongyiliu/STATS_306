---
subtitle: "Stats 306: Lecture 20"
title: "Debugging Continued; Profiling and Benchmarking"
author: "Mark Fredrickson"
output: 
  learnr::tutorial:
    progressive: true
    css: css/lecture.css
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
library(learnr)
library(tidyverse)

wikiart <- read_tsv("./data/WikiArt-Emotions-All.tsv.gz")
wikiart <- mutate(wikiart, Year = as.numeric(str_sub(Year, 1, 4)))

pivot_longer(wikiart, 
             matches(":"), 
             names_to = c("rating_type", "emotion"),
             names_sep = ":",
             values_to = "mean_annotation") -> wa_long

select(wa_long, Title, rating_type, emotion, mean_annotation)
wa_long <- mutate(wa_long,
                  emotion = str_replace(emotion, "\\s", ""))

filter(wa_long, str_detect(rating_type, "Art")) |>
  group_by(ID) |>
  arrange(desc(mean_annotation)) |>
  summarize(strongest_emotion = first(emotion),
            strongest_emotion_value = first(mean_annotation)) ->
  wa_art_strongest

wa_art_strong_year <- left_join(wikiart, wa_art_strongest, by = "ID")
```

## Review

* Wrapped up interation discussion
* Introduced `safely` as a way to capture error messages
* Debugging: finding errors in code
* General strategies: search, minimal cause, locate, fix and document

## Finding errors in the call stack 

(Run this in the console)

```{r}
f <- function(a) g(a)
g <- function(b) h(b)
h <- function(c) i(as.character(c))
i <- function(d) {
  if (!is.numeric(d)) {
    stop("`d` must be numeric", call. = FALSE)
  }
  d + 10
}
```
```{r eval = FALSE}
f("hello")
# Error: `d` must be numeric
```

Tools:

* `traceback()
* `library(rlang)` and `with_abort(f("hello"))` and `last_trace()`

## Using RStudio's debugger

```{r, eval = FALSE}
f(10)
```

## Using the browser

Add `browser()` in a function to pause execution and input commands:

* Any valid R code gets executed
* `help`: get available commands
* `next`, `n`: executes the next step in the function. 
* `step into`, `s`: step into the next function so you can explore it interactively.
* `finish`, or `f`: finishes execution of the current loop or function.
* `continue`, `c`: leaves interactive debugging and continues regular execution of the function.
* `stop`, `Q`: end debugging
* `where`: prints stack trace of active calls 

## Other tools

Open up `lecture20_debug.R`

## Debugging RMarkdown

If the problem only occurs when knitting, you will need to knit the document from the console using:

```{r, eval = FALSE}
rmarkdown::render("path/to/file.Rmd")
```

## Exercise

Open up `lecture20_debug_markdown.Rmd` and try to knit it.

Use `rmarkdown::render("lectures/lecture20_debug_markdown.Rmd` to find and fix the problem.

## Signals

R has several ways of signaling to the user that something is going on.

Messages:
```{r}
message("The value of pi is ", pi)
```

Warnings:
```{r}
warning("pi is not 3")
```

Errors:
```{r, eval = FALSE}
stop("You said pi was 3")
```

R has a `try`/`catch` mechanism for handling errors, though we won't go into detail.

## Debugging warnings


```{r}
f <- function(x) { 
  g(x - 1)
}
g <- function(x) {
  log(x) + 1
}

f(c(1, 2, -1, 10))
```

Promoting the warning into an error:
```{r, eval = FALSE}
options(warn = 2)
... do debugging ...
options(warn = 1) # back to normal

```

## Exercise

Where is the warning in this code coming from? Run this in the Console and use `options(warn = 2)` to help you debug it.

```{r}
pyramid_numbers <- function(x) {
  rep(x, x)
}

pyramid_numbers(1:3)

alternate_zeros <- function(x) {
  x * c(1, 0)
}

alternate_zeros(1:4)

both <- function(x) {
  alternate_zeros(pyramid_numbers(x))
}

both(1:3)
both(1:4)
both(1:5)
```

## Performance

The **performance** of a program is who quickly it can execute its answer.

Performance can depend on many things:

* Algorithmic complexity: how good is the algorithm (set of steps to complete computation)
* Implementation: R's **interpreter** is generally slower than **compiled** languages like C/C++ (but we get other advantages such as interactive development and metaprogramming)
* Resource usage: do we use lots of memory or disk space? 

Before we can fix it, we need to measure it.

## Profiling and Benchmarking

To measure our program, we will feed it some input and the either

* **Profile**: see how long each step (function) of the program takes.
* **Benchmark**: accurate measurement of time of particular function.

Big difference: profiling modifies the program slightly and may miss some functions. Benchmarking gives more real world results but limited it what it reports.

## Profiling in R

R includes a **sampling** profiler: it halts execution on a regular interval and takes a snapshot of the current state.

Profiling generally looks like this:

```{r, eval = FALSE}
Rprof() # turns on profiling
... run code ...
Rprof(NULL) # stop profiling
```

Output is placed (by default) in `Rprof.out`.

Example output from knitting this document:
```
sample.interval=20000
"ls" "FUN" "lapply" ".rs.objectsOnSearchPath" ".rs.getCompletionsSearchPath" "Reduce" ".rs.rpc.get_completions" 
"gsub" ".rs.fuzzyMatches" ".rs.getCompletionsSearchPath" "Reduce" ".rs.rpc.get_completions" 
"makeRestartList" "withRestarts" ".signalSimpleWarning" ".rs.normalizePath" "withCallingHandlers" "suppressWarnings" ".rs.rpc.get_completions" 
```

## Setting the interval

`Rprof` has an `interval` argument (default 0.02) for how often it should sample (in seconds).

Smaller interval means more precise, but runs slower.

## `profvis` library

The output can be hard to read, so we prefer to use the `profviz` library.

```{r, eval = FALSE}
library(profvis)
profvis(your_function())
```

Open up `lecture20_slow_sort.R`.

## Exercise

Open `lecture20_outer_product.R`. Profile using `run_two`. Where are the hot spots in this code?

## R's Memory Management

R has automatic memory management in the form of a **garbage collector**. When memory is no longer needed, it is collected and then reused for future computations.

For example:

```{r}
gc()[, 1:2]
m <- matrix(0, ncol = 1000, nrow = 1000)
gc()[, 1:2]
m <- 0
gc()[, 1:2]
```

The `gc` function runs automatically when R decides it needs to reclaim memory.

## GC in profiling

GC can be one of the more time intensive aspects of any R program. The special `<GC>` function in the profiling indicates that a garbage collection has been triggered.

Trying to eliminate "memory pressure" is one of the more important tasks in improving R performance.

## Example

Open `lecture20_outer_product.R`. Use `run_three` to provide the code. Which functions are requiring garbage collection inside the function.

## (Micro)benchmarking

A **benchmark** is a measurement we want to improve (or be sure not to go below).

A **microbenchmark** is a measurement of time that is very small. We usually make many microbenchmarks and combine them.

The `bench` library includes tools for microbenchmarking.

## The `bench` function

```{r}

v1 <- function(x) {
  result <- numeric(length(x))
  for (i in seq_along(x)) {
    result[i] <- (x[i] - mean(x))^2
  }
  sum(result) / length(x)
}

v2 <- function(x) {
  ss <- (x - mean(x))^2 |> sum()
  ss/length(x)
}

bm <- bench::mark(v1(1:1000), v2(1:1000))
```

## Results

```{r}
bm
```

* `mark` will try to run each function enough times to get to 5s of execution time total. 
* Times are in seconds, milliseconds (ms), microseconds($\mu s$) and nano-seconds (ns).
* Many times the `min` and `median` columns will be most instructive -- benchmarks can get contaminated by background processes.


## Exercise

Write a microbench mark to compare squaring a value versus multiplying a vector by itself. Which method is faster? What about raising a vector to the 8th power? How would you use this to write `intpow(x, power)`? Write this and benchmark the result.

```{r squares, exercise = TRUE}
x <- rnorm(1000)
```


## Checking output

`bench::mark` will want to make sure all results are the same. If thing differ slightly, you can set `check = FALSE`:

```{r, eval = FALSE}
bench::mark(c(1), c(one = 1))
# Error: Each result must equal the first result:
# ` c(1)` does not equal `c(one = 1)`
```
```{r}
bench::mark(c(1), c(one = 1), check = FALSE)
```

## Exercise

Compare the median function to using `quantile(x, q = 0.5)`. Which is faster?

```{r medq, exercise = TRUE}
```

## Keeping things in perspective

When microbenchmarking, we observe one operation takes 10ns and one takes 100ns. Is the 100ns version immediately preferable?

Only if it is going to be called a lot. 

Remember to keep things in perspective and use profiling to identify hot spots in actual evaluation and benchmarking to help improve those areas that need the most improvement.


